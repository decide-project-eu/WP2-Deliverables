---
title: "Shewhart Control Charts for Monitoring DLM Outputs"
author: "<Your Name>"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  word_document: default
  pdf_document: default
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Overview

Shewhart (often pronounced **“SHOO‑art”**) control charts are graphical tools used in statistical process control (SPC) to decide whether a sequence of measurements arises from a stable system or whether an **alarm** (signal of special‑cause variation) should be triggered.
When these charts are applied to *Dynamic Linear Models* (DLMs)—which supply real‑time predictions and residuals for streaming data—they serve as a low‑latency layer that converts probabilistic forecasts into crisp “in‑control/out‑of‑control” decisions.



## Assumptions of the Classical Shewhart Chart

1. **Independence** – Successive charted statistics (e.g., individual observations, subgroup means, or residuals) are mutually independent.
2. **Identically distributed** – Under the in‑control state they have common mean $\mu$ and variance $\sigma^{2}$.
3. **Normality (for $\bar{X}$ charts)** – The charted statistic is approximately Normal; this is guaranteed for subgroup means by the Central Limit Theorem.
4. **Process stability at start‑up** – The historical data used to estimate $\mu$ and $\sigma$ is itself in‑control.

Violating these assumptions (especially independence) inflates false‑alarm rates.  In a DLM context, independence can often be restored by charting *one‑step‑ahead forecast errors* rather than raw observations.



## Shewhart Three‑Sigma Limits

For a generic statistic $T_{t}$ that is in‑control with mean $\mu$ and standard deviation $\sigma$, the classical 3‑sigma chart uses

$$
\begin{aligned}
\text{Center Line (CL)}   &= \mu,\\
\text{Upper Control Limit (UCL)} &= \mu + k\,\sigma,\\
\text{Lower Control Limit (LCL)} &= \mu - k\,\sigma,\quad k = 3.
\end{aligned}
$$

The chart signals **out‑of‑control** when any plotted point falls outside $[\text{LCL},\,\text{UCL}]$.

### Linking to a DLM

Let $y_{t}$ be the observation at time $t$ and $\hat{y}_{t|t-1}$ its one‑step forecast with variance $Q_{t}$ from the Kalman filter.  Define the *standardized forecast error*

$$
 z_{t} = \frac{y_{t} - \hat{y}_{t|t-1}}{\sqrt{Q_{t}}} \sim \mathcal{N}(0,1)\quad (\text{in‑control}).
$$

Use $z_{t}$ as $T_{t}$ in the formula above: CL = 0, UCL = 3, LCL = –3.  This produces an **Individuals ($X$) chart** that is perfectly tuned to the predictive distribution generated by the DLM.



## The Four *Montgomery* (Western Electric) Rules

Douglas C. Montgomery’s *Introduction to Statistical Quality Control* promotes four supplementary rules that detect subtler patterns than a single 3‑sigma point:

| Rule | Description                                                                 | Rationale                 |
| ---- | --------------------------------------------------------------------------- | ------------------------- |
| 1    | **One point outside $\pm3\sigma$.**                                         | Large, sudden shift.      |
| 2    | **Two of three consecutive points outside $\pm2\sigma$ (same side).**       | Moderate sustained shift. |
| 3    | **Four of five consecutive points outside $\pm1\sigma$ (same side).**       | Small sustained shift.    |
| 4    | **Eight (or nine) consecutive points on the same side of the center line.** | Persistent drift in mean. |

A violation of *any* rule constitutes an alarm.  For Gaussian, independent data these four rules jointly cap the false‑signal probability near 0.0027 per point while substantially increasing sensitivity to drifts and small shifts.



## Illustrative Example

Here we will look at the data set *DLM_preprocessed_data__NEW_WithoutActivity.RDS*, in which the variables "consumption_liters", "visitswent", "visits", and "DrinkingSpeed" have been filtered using a multivariate DLM. 

Furthermore, we will source the script *Functions for monitoring and filtering.R*. 

```{r}
res <- readRDS("DLM_preprocessed_data__NEW_WithoutActivity.RDS")
source('Functions for monitoring and filtering.R')
```

### Checking for auto-correlation

The DLM was applied to the variables "consumption_liters", "visitswent", "visits", "DrinkingSpeed". We can use the *acf* to confirm that the raw data do not live up to the assumption of each observation in the time series being mutually independent, i.e. there being no auto-correlations.

```{r}
relevant.names <- c("consumption_liters", "visitswoent", "visits", "DrinkingSpeed")

par(mfrow=c(1,4))
for(name in relevant.names){
  acf(res[,name], main=name)
}
```
We can use the same function to confirm that the standardized forecast errors produced by the DLM has no auto-correlations.

```{r}
ut.names <- c("ut_consumption_liters", "ut_visitswent", "ut_visits", "ut_DrinkingSpeed")

par(mfrow=c(1,4))
for(name in ut.names){
  acf(res[,name], main=name)
}
```

### Applying montgomery rules

#### Illustrative examples

Initially, we can randomly select a healthy and a sick calf to apply the 4 Montomery rules to for illustrative purposes. For this illustration, we will only apply the function to the *ut_consumption_litres* column. 

In theory, the standardized forecast errors should follow a standard normal distribution with a mean of 0 and a standard deviation of 0. Therefore, we will first apply the *alarmsMontgomery* function with these settings. 

```{r}
# Select a random healthy calf
set.seed(42)
healthy.calves <- unique(res$calf.herd[which(res$AnySickness == 0)])
healthy.calves <- sample(x = healthy.calves, size = 1)

# Select a random sick calf
set.seed(42)
sick.calves <- unique(res$calf.herd[which(res$AnySickness == 1)])
sick.calves <- sample(x = sick.calves, size = 1)

# Plot the selected calves
calves <- c(healthy.calves, sick.calves)

for(calf in calves){
    calf.set <- subset(res, res$calf.herd == calf)
    alarmsMontgomery(k = calf.set[,'ut_consumption_liters'], cl = 0, SD = 1, Ylim = c(-4,4), Main = paste(calf, '|', name), plot.it = TRUE)
}

```
As is seen, the standardized forecast errors do not follow a proper standard normal distribution in this case. Thus, we can estimate the mean and standard deviation from the healthy calves.

```{r}
# Select only the healthy calves
healthy.set <- subset(res, res$calf.herd %in% healthy.calves)
Mean <- mean(healthy.set[,'ut_consumption_liters'])
SD <- sd(healthy.set[,'ut_consumption_liters'])

# Plot the selected calves
calves <- c(healthy.calves, sick.calves)

for(calf in calves){
    calf.set <- subset(res, res$calf.herd == calf)
    alarmsMontgomery(k = calf.set[,'ut_consumption_liters'], cl = Mean, SD = SD, Ylim = NA, Main = paste(calf, '|', name), plot.it = TRUE)
}

```

This time we get many alarms for the sick calf, but not for the healthy calf. 

#### Assesing performance

First, we will see what kind of performance we get if we apply the 4 Montgomery rules to each of the *ut* columns with the assumptions of standard normal distributions. After applying the *alarmsMontgomery* function to the forecast errors, we use the *getPerformance* to asses the performance of the alarms. Here the observations are the values in the *SickOrHealthy* column.



```{r}

performance.all <- data.frame()
for(name in ut.names){
    alarms <- alarmsMontgomery(k = res[,name], cl = 0, SD = 1, Ylim = NA, Main = paste(calf, '|', name), plot.it = FALSE)
    for(i in 1:ncol(alarms)){
      performance <- getPerformance(observations = res$SickOrHealthy, alarms[,i])
      performance <- cbind('Name'=name, 'Rule'=colnames(alarms)[i], performance)
      performance.all <- rbind(performance.all, performance)
    }
    
}

print(summary(as.numeric(performance.all$MMA)))
print(performance.all[which(performance.all$MMA == max(performance.all$MMA)),])
```
We see that the highest major mean accuracy (MMA) for names/rules combination is 53.5 %. 

Next we can now test the performance if we estimate the distribution from healthy calves. To do this, we will use the same per-herd cross-validation scheme as was used when training and applying the DLM.

```{r}

# Iterate over each of the names
performance.all <- data.frame()
for(name in ut.names){
  
  alarms.all.name <- data.frame()
  for(herd in unique(res$Herd)){
    
    # Get the data from just this herd
    herd.set <- subset(res, res$Herd == herd)
    
    # Use only the healthy calves from the other herds as a learning set
    learning.set  <- subset(res, res$Herd != herd)
    learning.set <- subset(learning.set, learning.set$calf.herd %in% healthy.calves )
    
    #Learn the distribution
    Mean <- mean(learning.set[,name])
    SD <- sd(learning.set[,name])
    
    # Make the alarms for this herd
    alarms <- alarmsMontgomery(k = herd.set[,name], cl = Mean, SD = SD, Ylim = NA, Main = paste(calf, '|', name), plot.it = FALSE)
    alarms.all.name <- rbind(alarms.all.name, alarms)
    
  }
  
  # Get the performance for each rule on this name
   for(i in 1:ncol(alarms.all.name)){
      performance <- getPerformance(observations = res$SickOrHealthy, alarms.all.name[,i])
      performance <- cbind('Name'=name, 'Rule'=colnames(alarms.all.name)[i], performance)
      performance.all <- rbind(performance.all, performance)
    }
}

print(summary(as.numeric(performance.all$MMA)))

print(performance.all[which(performance.all$MMA == max(performance.all$MMA)),])
```
We see that the best MMA has increased to 59 %. 

## Practical Considerations

* **Autocorrelation** – If the DLM’s residuals are not white noise, consider charting batches or using EWMA/CUSUM charts instead.
* **Phase I vs Phase II** – Estimate $\mu$ and $\sigma$ from a stable *Phase I* period; then lock them for monitoring (*Phase II*).
* **Multiple streams** – For multivariate DLM outputs, build separate Shewhart charts.



## References

1. Shewhart, W. A. (1931). *Economic Control of Quality of Manufactured Product.*
2. Montgomery, D. C. (2020). *Introduction to Statistical Quality Control* (9th ed.). Wiley.
3. West, M. & Harrison, J. (1997). *Bayesian Forecasting and Dynamic Models.*
